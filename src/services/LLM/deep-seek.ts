import { appConfig } from '../../config/index.js';
import type { LLMService, LLMMessage, LLMChatOptions, LLMChatResponse } from './llm-service.js';

/**
 * DeepSeek API è¯·æ±‚å‚æ•°
 */
export interface DeepSeekRequest {
  model: string;
  messages: LLMMessage[];
  max_tokens?: number;
  temperature?: number;
  stream?: boolean;
}

/**
 * DeepSeek API å“åº”
 */
export interface DeepSeekResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

/**
 * DeepSeek API é”™è¯¯å“åº”
 */
export interface DeepSeekError {
  error: {
    message: string;
    type: string;
    code?: string;
  };
}

/**
 * DeepSeek æœåŠ¡ç±»
 * æä¾›ä¸ DeepSeek OpenAPI çš„äº¤äº’åŠŸèƒ½
 */
export class DeepSeekService implements LLMService {
  private apiKey: string;
  private baseUrl: string;
  private model: string;
  private maxTokens: number;
  private temperature: number;
  private timeout: number;

  constructor() {
    const config = appConfig.deepSeek;
    this.apiKey = config.apiKey;
    this.baseUrl = config.baseUrl;
    this.model = config.model;
    this.maxTokens = config.maxTokens;
    this.temperature = config.temperature;
    this.timeout = config.timeout;
  }

  /**
   * æ£€æŸ¥ API é…ç½®æ˜¯å¦æœ‰æ•ˆ
   */
  private validateConfig(): boolean {
    if (!this.apiKey) {
      console.error('âŒ DeepSeek API Key æœªé…ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY');
      return false;
    }
    return true;
  }

  /**
   * å‘é€èŠå¤©è¯·æ±‚åˆ° DeepSeek API
   * @param messages æ¶ˆæ¯æ•°ç»„
   * @param options å¯é€‰å‚æ•°
   * @returns API å“åº”
   */
  async chat(messages: LLMMessage[], options: LLMChatOptions = {}): Promise<LLMChatResponse> {
    if (!this.validateConfig()) {
      throw new Error('DeepSeek API é…ç½®æ— æ•ˆ');
    }

    const requestBody: DeepSeekRequest = {
      model: options.model || this.model,
      messages,
      max_tokens: options.maxTokens || this.maxTokens,
      temperature: options.temperature || this.temperature,
      stream: false,
    };

    try {
      console.log(`ğŸ¤– å‘é€è¯·æ±‚åˆ° DeepSeek API...`);
      
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
        },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(this.timeout),
      });

      if (!response.ok) {
        const errorData: DeepSeekError = await response.json();
        throw new Error(`DeepSeek API é”™è¯¯: ${errorData.error.message}`);
      }

      const data: DeepSeekResponse = await response.json();
      console.log(`âœ… DeepSeek API è¯·æ±‚æˆåŠŸ`);
      
      return {
        success: true,
        content: data.choices[0]?.message?.content || '',
        usage: {
          promptTokens: data.usage.prompt_tokens,
          completionTokens: data.usage.completion_tokens,
          totalTokens: data.usage.total_tokens,
        },
      };
    } catch (error) {
      console.error(`âŒ DeepSeek API è¯·æ±‚å¤±è´¥:`, error);
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  /**
   * å¸¦ç³»ç»Ÿæç¤ºçš„èŠå¤©
   * @param systemPrompt ç³»ç»Ÿæç¤º
   * @param userMessage ç”¨æˆ·æ¶ˆæ¯
   * @param options å¯é€‰å‚æ•°
   * @returns ç”Ÿæˆçš„æ–‡æœ¬
   */
  async chatWithSystem(systemPrompt: string, userMessage: string, options: LLMChatOptions = {}): Promise<string> {
    const messages: LLMMessage[] = [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userMessage }
    ];
    
    const response = await this.chat(messages, options);
    return response.content || '';
  }

  /**
   * æµ‹è¯• API è¿æ¥
   * @returns æ˜¯å¦è¿æ¥æˆåŠŸ
   */
  async testConnection(): Promise<boolean> {
    try {
      const messages: LLMMessage[] = [
        { role: 'user', content: 'Hello' }
      ];
      const response = await this.chat(messages,  { maxTokens: 10 });
      const content =  response.content || '';
      return content.length > 0;
    } catch (error) {
      console.error(`âŒ DeepSeek API è¿æ¥æµ‹è¯•å¤±è´¥:`, error);
      return false;
    }
  }
}